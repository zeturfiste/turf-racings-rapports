# -*- coding: utf-8 -*-
"""
Re-scraping des courses manquantes ZEturf.

- Lit verification_report.txt pour connaÃ®tre les courses manquantes
- Reconstruit les URLs et tÃ©lÃ©charge les HTML manquants
- Concurrency fixe = 100, pause fixe = 30s entre lots
- Travail par annÃ©e, dans l'ordre
- Commit + push par annÃ©e (rÃ©pertoires resultats-et-rapports/YYYY)
- Support d'une liste d'annÃ©es (--years) pour matrix GitHub Actions
- ArrÃªt intelligent avant 6h: on n'entame pas une annÃ©e si on sait qu'on
  ne pourra pas la finir (estimation basÃ©e sur 200 courses/min + overhead).
"""

import os
import re
import time
import asyncio
import aiohttp
from pathlib import Path
from collections import defaultdict
from datetime import datetime
import subprocess
import shutil

# =========================
# Configuration
# =========================
BASE = "https://www.zeturf.fr"
REPO_ROOT = "resultats-et-rapports"

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/126.0 Safari/537.36"
    ),
    "Accept-Language": "fr-FR,fr;q=0.9,en;q=0.8",
}

# Concurrency fixe : nombre de requÃªtes HTTP en parallÃ¨le
CONCURRENCY = 100

# Pause fixe entre lots (en secondes)
SLEEP_BETWEEN_LOTS = 30

# Seuils disque
WARN_DISK_GB = 5
CRITICAL_DISK_GB = 2
YEAR_SKIP_DISK_GB = 3

# Estimation temps / annÃ©e
MAX_JOB_MINUTES = 360.0          # limite GitHub Actions (6h)
SAFETY_MARGIN_MINUTES = 15.0     # marge de sÃ©curitÃ© (commit, fin de job, etc.)
AVG_COURSES_PER_MINUTE = 200.0   # ton observation: ~200 courses/min
PER_YEAR_OVERHEAD_MIN = 5.0      # overhead fixe par annÃ©e (git, etc.)
JOB_START_ENV = "JOB_START_EPOCH"

# =========================
# Disk monitoring
# =========================
def get_disk_space_gb() -> float:
    """Retourne l'espace disque disponible en GB."""
    stat = shutil.disk_usage("/")
    return stat.free / (1024 ** 3)


def check_disk_space_critical() -> bool:
    """VÃ©rifie si l'espace disque est critique (< CRITICAL_DISK_GB)."""
    free_gb = get_disk_space_gb()
    if free_gb < CRITICAL_DISK_GB:
        print(f"\nâš ï¸  ALERTE: Espace disque critique: {free_gb:.2f} GB restants")
        print("ArrÃªt du scraping pour Ã©viter saturation...")
        return True
    return False

# =========================
# Time estimation helpers
# =========================
def estimate_year_minutes(year: str, year_courses: int) -> float:
    """Estimation de la durÃ©e pour une annÃ©e, en minutes."""
    # Simple: 200 courses/min + overhead fixe
    return (year_courses / AVG_COURSES_PER_MINUTE) + PER_YEAR_OVERHEAD_MIN


def can_process_year(year: str, year_courses: int) -> bool:
    """
    DÃ©cide si on a assez de temps pour traiter cette annÃ©e
    avant dâ€™atteindre la limite de 6h du job GitHub Actions.

    Utilise la variable dâ€™environnement JOB_START_EPOCH dÃ©finie
    dans le workflow (premier step du job).
    """
    start_epoch_str = os.environ.get(JOB_START_ENV)
    if not start_epoch_str:
        # On ne connait pas lâ€™heure de dÃ©but -> on ne force pas de limite.
        return True

    try:
        start_epoch = float(start_epoch_str)
    except ValueError:
        return True

    now = time.time()
    elapsed_min = (now - start_epoch) / 60.0
    remaining_min = MAX_JOB_MINUTES - elapsed_min - SAFETY_MARGIN_MINUTES

    est_min = estimate_year_minutes(year, year_courses)

    print(
        f"â±ï¸  Estimation pour l'annÃ©e {year}: {year_courses} courses "
        f"â†’ ~{est_min:.1f} min. "
        f"Temps Ã©coulÃ©: ~{elapsed_min:.1f} min, "
        f"marge restante avant 6h: ~{remaining_min:.1f} min."
    )

    if remaining_min <= 0:
        print("âš ï¸  Plus de marge suffisante, arrÃªt immÃ©diat.")
        return False

    if est_min > remaining_min:
        print(
            f"âš ï¸  On NE traite PAS l'annÃ©e {year}, "
            "car on estime qu'on n'aura pas le temps de la terminer avant la limite 6h."
        )
        return False

    return True

# =========================
# Path helpers
# =========================
def get_date_directory(date_str: str) -> Path:
    """Retourne le chemin du dossier de la date: YYYY/MM/YYYY-MM-DD/."""
    dt = datetime.strptime(date_str, "%Y-%m-%d")
    year = dt.strftime("%Y")
    month = dt.strftime("%m")
    return Path(REPO_ROOT) / year / month / date_str


def save_html(filepath: Path, html: str) -> None:
    """Sauvegarde le HTML dans le fichier, en crÃ©ant les dossiers au besoin."""
    filepath.parent.mkdir(parents=True, exist_ok=True)
    filepath.write_text(html, encoding="utf-8")

# =========================
# Parse verification report
# =========================
def parse_missing_courses(report_path: Path = Path("verification_report.txt")):
    """
    Parse le rapport de vÃ©rification pour extraire les courses manquantes.

    Format attendu dans le rapport:
        DATE: 2006-04-16 - STATUS: INCOMPLETE
        âŒ R1-auteuil/R1C2-prix-du-president-de-la-republique.html

    Returns:
        dict[year][date] = [(reunion_slug, course_file), ...]
    """
    if not report_path.exists():
        print(f"âŒ Fichier {report_path} introuvable")
        return {}

    missing = defaultdict(lambda: defaultdict(list))
    current_date = None

    with report_path.open("r", encoding="utf-8") as f:
        for raw_line in f:
            line = raw_line.strip()

            # En-tÃªte de date
            if line.startswith("DATE:") and "STATUS:" in line:
                match = re.search(r"DATE:\s*(\d{4}-\d{2}-\d{2})", line)
                if match:
                    current_date = match.group(1)

            # Lignes de courses manquantes
            elif current_date and line.startswith("âŒ") and "/" in line and ".html" in line:
                # Format: âŒ R1-auteuil/R1C2-prix-xxx.html
                match = re.search(r"âŒ\s*([^/]+)/([^/]+\.html)", line)
                if match:
                    reunion_slug = match.group(1)
                    course_file = match.group(2)
                    year = current_date[:4]
                    missing[year][current_date].append((reunion_slug, course_file))

    # On renvoie un dict classique pour figer l'ordre
    return dict(missing)

# =========================
# URL reconstruction
# =========================
def build_course_url(date_str: str, reunion_slug: str, course_file: str) -> str:
    """
    Reconstruit l'URL de la course depuis le nom de fichier.

    Ex:
      date = 2006-04-16
      reunion_slug = "R1-auteuil"
      course_file  = "R1C2-prix-du-president-de-la-republique.html"

    â†’ https://www.zeturf.fr/fr/course/2006-04-16/R1C2-auteuil-prix-du-president-de-la-republique
    """
    hippodrome = reunion_slug.split("-", 1)[1] if "-" in reunion_slug else reunion_slug
    course_slug = course_file[:-5] if course_file.endswith(".html") else course_file

    if "-" in course_slug:
        code_part, title_part = course_slug.split("-", 1)
    else:
        code_part, title_part = course_slug, ""

    if title_part:
        url_suffix = f"{code_part}-{hippodrome}-{title_part}"
    else:
        url_suffix = f"{code_part}-{hippodrome}"

    return f"{BASE}/fr/course/{date_str}/{url_suffix}"

# =========================
# HTTP fetching (concurrent)
# =========================
async def fetch_course(session: aiohttp.ClientSession, url: str, retries: int = 3) -> tuple[str, int]:
    """RÃ©cupÃ¨re le HTML d'une course. Retourne (html, status_code)."""
    last_exc = None
    for attempt in range(retries):
        try:
            async with session.get(
                url,
                headers=HEADERS,
                timeout=aiohttp.ClientTimeout(total=30),
            ) as resp:
                html = await resp.text()
                return html, resp.status
        except asyncio.TimeoutError as e:
            last_exc = e
            if attempt == retries - 1:
                raise
            await asyncio.sleep(2 * (attempt + 1))
        except Exception as e:
            last_exc = e
            if attempt == retries - 1:
                raise
            await asyncio.sleep(2 * (attempt + 1))

    if last_exc:
        raise last_exc
    raise RuntimeError("fetch_course failed without exception")


async def _scrape_one_course(
    sem: asyncio.Semaphore,
    session: aiohttp.ClientSession,
    date_str: str,
    reunion_slug: str,
    course_file: str,
    filepath: Path,
):
    """
    TÃ¢che individuelle pour une course.
    Retourne (status, error_msg | None).
    """
    async with sem:
        url = build_course_url(date_str, reunion_slug, course_file)
        try:
            html, status = await fetch_course(session, url)

            if status == 200:
                save_html(filepath, html)
                print(f"      âœ“ {date_str} {reunion_slug}/{course_file}")
                return status, None
            else:
                msg = f"{date_str} {reunion_slug}/{course_file} (HTTP {status})"
                print(f"      âœ— {msg}")
                return status, msg

        except Exception as e:
            msg = f"{date_str} {reunion_slug}/{course_file} ({str(e)[:80]})"
            print(f"      âœ— {msg}")
            return None, msg

# =========================
# Scraping d'une annÃ©e, en lots successifs
# =========================
async def scrape_year(year: str, dates_courses: dict) -> None:
    """
    Scrape toutes les courses manquantes pour une annÃ©e, en respectant l'ordre
    date â†’ rÃ©union â†’ course, avec une concurrency fixe et des lots successifs.
    """
    print(f"\n{'=' * 80}")
    print(f"ANNÃ‰E {year}")
    print(f"{'=' * 80}\n")

    free_gb = get_disk_space_gb()
    print(f"ğŸ’¾ Espace disque disponible: {free_gb:.2f} GB")

    if free_gb < YEAR_SKIP_DISK_GB:
        print("âš ï¸  Espace insuffisant pour traiter cette annÃ©e, on saute.")
        return

    # Aplatir toutes les courses de l'annÃ©e dans l'ordre
    all_courses = []
    for date_str in sorted(dates_courses.keys()):
        for reunion_slug, course_file in dates_courses[date_str]:
            date_dir = get_date_directory(date_str)
            reunion_dir = date_dir / reunion_slug
            filepath = reunion_dir / course_file
            all_courses.append((date_str, reunion_slug, course_file, filepath))

    total_courses = len(all_courses)
    print(f"ğŸ“Š {total_courses} courses Ã  rÃ©cupÃ©rer pour {year}")

    if total_courses == 0:
        return

    pending = list(all_courses)

    stats = {
        "total": total_courses,
        "success": 0,
        "failed_429": 0,
        "failed_other": 0,
        "lots": 0,
    }

    lot_index = 0

    async with aiohttp.ClientSession() as session:
        while pending:
            if check_disk_space_critical():
                print("âš ï¸  ArrÃªt pour manque d'espace disque.")
                break

            lot_index += 1
            stats["lots"] += 1

            free_gb = get_disk_space_gb()
            lot_size = min(CONCURRENCY, len(pending))
            current_lot = pending[:lot_size]
            pending = pending[lot_size:]

            first_idx = total_courses - len(pending) - lot_size + 1
            last_idx = total_courses - len(pending)

            print(
                f"\n  ğŸ§© Lot #{lot_index}: courses {first_idx}-{last_idx}/{total_courses} "
                f"(taille lot: {lot_size}, concurrency: {CONCURRENCY})"
            )
            print(f"  ğŸ’¾ Espace libre avant lot: {free_gb:.2f} GB")
            print(f"  â±ï¸  Pause fixe entre lots: {SLEEP_BETWEEN_LOTS}s")

            sem = asyncio.Semaphore(CONCURRENCY)
            tasks = [
                _scrape_one_course(sem, session, date_str, reunion_slug, course_file, filepath)
                for (date_str, reunion_slug, course_file, filepath) in current_lot
            ]

            lot_start = time.time()
            lot_success = 0
            lot_429 = 0
            lot_other_errors = 0
            retry_429 = []

            for (date_str, reunion_slug, course_file, filepath), coro in zip(
                current_lot, asyncio.as_completed(tasks)
            ):
                status, err = await coro
                if status == 200:
                    lot_success += 1
                elif status == 429:
                    lot_429 += 1
                    retry_429.append((date_str, reunion_slug, course_file, filepath))
                else:
                    if err is not None:
                        lot_other_errors += 1

            lot_duration = time.time() - lot_start

            stats["success"] += lot_success
            stats["failed_429"] += lot_429
            stats["failed_other"] += lot_other_errors

            if retry_429:
                print(f"  ğŸ” {lot_429} courses avec 429 seront retentÃ©es au lot suivant.")
                pending = retry_429 + pending

            print(
                f"  â±ï¸  Lot #{lot_index} terminÃ© en {lot_duration:.2f}s "
                f"(succÃ¨s: {lot_success}, 429: {lot_429}, autres erreurs: {lot_other_errors})"
            )
            print(f"  ğŸ’¾ Espace libre aprÃ¨s lot: {get_disk_space_gb():.2f} GB")

            if pending:
                print(f"  â³ Pause de {SLEEP_BETWEEN_LOTS}s avant le lot suivant...")
                await asyncio.sleep(SLEEP_BETWEEN_LOTS)

    print(f"\n{'=' * 80}")
    print(f"RÃ‰SUMÃ‰ ANNÃ‰E {year}")
    print(f"{'=' * 80}")
    print(f"  Total prÃ©vu:      {stats['total']}")
    print(f"  âœ“ SuccÃ¨s:         {stats['success']}")
    print(f"  âœ— 429 (non OK):   {stats['failed_429']}")
    print(f"  âœ— Autres erreurs: {stats['failed_other']}")
    print(f"  ğŸ” Nombre de lots: {stats['lots']}")
    print(f"  ğŸ’¾ Espace final:   {get_disk_space_gb():.2f} GB")
    print(f"{'=' * 80}\n")

# =========================
# Git operations
# =========================
def git_commit_year(year: str) -> None:
    """Commit et push les changements pour l'annÃ©e donnÃ©e."""
    print(f"\nğŸ“¤ Git commit pour l'annÃ©e {year}...")

    target_path = f"{REPO_ROOT}/{year}"

    try:
        subprocess.run(
            ["git", "config", "user.name", "GitHub Actions Bot"],
            check=True,
        )
        subprocess.run(
            ["git", "config", "user.email", "actions@github.com"],
            check=True,
        )

        # Y a-t-il des changements pour cette annÃ©e ?
        status = subprocess.run(
            ["git", "status", "--porcelain", target_path],
            capture_output=True,
            text=True,
            check=True,
        )
        if not status.stdout.strip():
            print("  â„¹ï¸  Aucun changement pour cette annÃ©e")
            return

        # Stage uniquement cette annÃ©e
        subprocess.run(["git", "add", target_path], check=True)

        files_changed = status.stdout.count("\n")
        commit_msg = f"Re-scrape: {year} - {files_changed} fichiers modifiÃ©s/ajoutÃ©s"
        timestamp_msg = f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}"

        subprocess.run(
            ["git", "commit", "-m", commit_msg, "-m", timestamp_msg],
            check=True,
        )

        # IntÃ©grer les commits des autres jobs sans rÃ©Ã©crire l'historique
        subprocess.run(
            ["git", "pull", "--no-rebase", "--no-edit", "origin", "main"],
            check=False,
        )

        # Push
        subprocess.run(["git", "push", "origin", "HEAD:main"], check=True)

        print(f"  âœ“ AnnÃ©e {year} committÃ©e et pushÃ©e ({files_changed} fichiers)\n")

    except subprocess.CalledProcessError as e:
        print(f"  âœ— Erreur Git: {e}\n")

# =========================
# Main orchestrator
# =========================
async def main() -> None:
    import argparse

    parser = argparse.ArgumentParser(
        description="Re-scrape intelligent des courses ZEturf manquantes"
    )
    parser.add_argument(
        "--max-courses",
        type=int,
        default=None,
        help="(Optionnel) Limite globale de courses Ã  traiter (approx, par annÃ©es entiÃ¨res)",
    )
    parser.add_argument(
        "--years",
        type=str,
        nargs="*",
        help="(Optionnel) Liste d'annÃ©es Ã  traiter (ex: 2008 2011 2014). "
             "Si non fourni, toutes les annÃ©es prÃ©sentes dans verification_report.txt.",
    )
    args = parser.parse_args()

    print("=" * 80)
    print("RE-SCRAPING DIRECT DES COURSES MANQUANTES")
    print("=" * 80 + "\n")

    free_gb = get_disk_space_gb()
    print(f"ğŸ’¾ Espace disque initial: {free_gb:.2f} GB\n")

    if free_gb < WARN_DISK_GB:
        print("âš ï¸  WARNING: Espace disque faible! RecommandÃ©: > 5GB")
        print("Continuation avec prudence...\n")

    missing_by_year = parse_missing_courses()

    if not missing_by_year:
        print("âœ“ Aucune course manquante dÃ©tectÃ©e\n")
        return

    # Filtrage par annÃ©es si fourni
    if args.years:
        allowed = set(args.years)
        missing_by_year = {
            y: data for (y, data) in missing_by_year.items() if y in allowed
        }
        if not missing_by_year:
            print(f"â„¹ï¸  Aucune course manquante pour les annÃ©es demandÃ©es: {sorted(allowed)}")
            return
        print(f"â¡ï¸  AnnÃ©es filtrÃ©es: {', '.join(sorted(missing_by_year.keys()))}\n")

    total_courses = sum(
        len(courses)
        for year_data in missing_by_year.values()
        for courses in year_data.values()
    )
    print(f"ğŸ“Š {len(missing_by_year)} annÃ©es avec courses manquantes")
    print(f"ğŸ“Š {total_courses} courses manquantes au total\n")

    courses_planned = 0
    for year in sorted(missing_by_year.keys()):
        year_courses = sum(len(c) for c in missing_by_year[year].values())

        if args.max_courses and courses_planned >= args.max_courses:
            print(f"âš ï¸  Limite globale atteinte (~{courses_planned} courses planifiÃ©es).")
            break

        # VÃ©rifier si on a assez de temps pour cette annÃ©e
        if not can_process_year(year, year_courses):
            print("â¹ï¸  ArrÃªt anticipÃ© pour Ã©viter de gaspiller un run presque terminÃ©.")
            break

        print(f"â¡ï¸  Traitement de l'annÃ©e {year} ({year_courses} courses prÃ©vues)")
        await scrape_year(year, missing_by_year[year])

        git_commit_year(year)

        courses_planned += year_courses

    print("\n" + "=" * 80)
    print("SCRAPING TERMINÃ‰")
    print(f"ğŸ’¾ Espace disque final: {get_disk_space_gb():.2f} GB")
    print("=" * 80)


if __name__ == "__main__":
    asyncio.run(main())
