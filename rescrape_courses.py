# -*- coding: utf-8 -*-
"""
Script de re-scraping intelligent des courses manquantes ZEturf
- Parse verification_report.txt pour identifier les courses manquantes
- Reconstruit les URLs directement depuis les noms de fichiers
- Auto-ajustement du batch size avec m√©morisation de la limite safe
- Monitoring de l'espace disque
- Commit par ann√©e
"""
import os
import re
import asyncio
import aiohttp
from pathlib import Path
from collections import defaultdict
from datetime import datetime
import subprocess

# =========================
# Configuration
# =========================
BASE = "https://www.zeturf.fr"
REPO_ROOT = "resultats-et-rapports"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/126.0 Safari/537.36",
    "Accept-Language": "fr-FR,fr;q=0.9,en;q=0.8",
}

# Rate limiting avec auto-ajustement
INITIAL_BATCH_SIZE = 200
MIN_BATCH_SIZE = 10
MAX_SAFE_BATCH_SIZE = None  # Sera d√©fini quand un 429 est d√©tect√©
CONSECUTIVE_THRESHOLD = 3   # Nombre de succ√®s avant augmentation
INCREMENT_STEP = 10         # Pas d'augmentation/r√©duction

# =========================
# Disk monitoring
# =========================
def get_disk_space_gb():
    """Retourne l'espace disque disponible en GB"""
    import shutil
    stat = shutil.disk_usage('/')
    return stat.free / (1024**3)

def check_disk_space_critical():
    """V√©rifie si l'espace disque est critique (< 2GB)"""
    free_gb = get_disk_space_gb()
    if free_gb < 2:
        print(f"\n‚ö†Ô∏è  ALERTE: Espace disque critique: {free_gb:.2f} GB restants")
        print("Arr√™t du scraping pour √©viter saturation...")
        return True
    return False

# =========================
# Path helpers
# =========================
def get_date_directory(date_str: str) -> Path:
    """Retourne le chemin du dossier de la date: YYYY/MM/YYYY-MM-DD/"""
    dt = datetime.strptime(date_str, "%Y-%m-%d")
    year = dt.strftime("%Y")
    month = dt.strftime("%m")
    return Path(REPO_ROOT) / year / month / date_str

def save_html(filepath: Path, html: str):
    """Sauvegarde le HTML dans le fichier"""
    filepath.parent.mkdir(parents=True, exist_ok=True)
    filepath.write_text(html, encoding="utf-8")

# =========================
# Parse verification report
# =========================
def parse_missing_courses(report_path: Path = Path("verification_report.txt")):
    """
    Parse le rapport de v√©rification pour extraire les courses manquantes.
    
    Format attendu dans le rapport:
        DATE: 2006-04-16 - STATUS: INCOMPLETE
        ‚ùå R1-auteuil/R1C2-prix-du-president-de-la-republique.html
    
    Returns: 
        dict[year][date] = [(reunion_slug, course_file), ...]
    """
    if not report_path.exists():
        print(f"‚ùå Fichier {report_path} introuvable")
        return {}
    
    missing = defaultdict(lambda: defaultdict(list))
    current_date = None
    
    with open(report_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            
            # D√©tecter l'en-t√™te de date
            if line.startswith("DATE:") and "STATUS:" in line:
                match = re.search(r"DATE:\s*(\d{4}-\d{2}-\d{2})", line)
                if match:
                    current_date = match.group(1)
            
            # D√©tecter les courses manquantes uniquement
            elif current_date and line.startswith("‚ùå") and "/" in line and ".html" in line:
                # Format: ‚ùå R1-auteuil/R1C2-prix-du-president-de-la-republique.html
                match = re.search(r"‚ùå\s*([^/]+)/([^/]+\.html)", line)
                if match:
                    reunion_slug = match.group(1)
                    course_file = match.group(2)
                    year = current_date[:4]
                    missing[year][current_date].append((reunion_slug, course_file))
    
    return dict(missing)

# =========================
# URL reconstruction
# =========================
def build_course_url(date_str: str, reunion_slug: str, course_file: str) -> str:
    """
    Reconstruit l'URL de la course depuis le nom de fichier.
    
    Ex: date=2006-04-16, reunion=R1-auteuil, file=R1C2-prix-du-president-de-la-republique.html
    ‚Üí https://www.zeturf.fr/fr/course/2006-04-16/R1C2-auteuil-prix-du-president-de-la-republique
    """
    # Extract hippodrome from reunion_slug: "R1-auteuil" ‚Üí "auteuil"
    hippodrome = reunion_slug.split("-", 1)[1] if "-" in reunion_slug else reunion_slug
    
    # Remove .html extension
    course_slug = course_file.replace(".html", "")
    
    # URL format: /fr/course/DATE/CODE-HIPPODROME-TITLE
    # Ex: R1C2-auteuil-prix-du-president-de-la-republique
    url = f"{BASE}/fr/course/{date_str}/{course_slug[:course_slug.find('-')]}-{hippodrome}-{course_slug[course_slug.find('-')+1:]}"
    
    return url

# =========================
# HTTP fetching
# =========================
async def fetch_course(session: aiohttp.ClientSession, url: str, retries=3) -> tuple[str, int]:
    """
    R√©cup√®re le HTML d'une course.
    
    Returns: (html, status_code)
    """
    for attempt in range(retries):
        try:
            async with session.get(url, headers=HEADERS, timeout=aiohttp.ClientTimeout(total=30)) as resp:
                html = await resp.text()
                return html, resp.status
        except asyncio.TimeoutError:
            if attempt == retries - 1:
                raise
            await asyncio.sleep(2 * (attempt + 1))
        except Exception as e:
            if attempt == retries - 1:
                raise
            await asyncio.sleep(2 * (attempt + 1))

# =========================
# Batch scraping avec auto-ajustement
# =========================
async def scrape_courses_batch(session: aiohttp.ClientSession, courses: list, batch_size: int):
    """
    Scrape un batch de courses avec d√©tection du rate limit.
    
    Args:
        courses: [(date, reunion_slug, course_file, filepath), ...]
        batch_size: Nombre de courses √† traiter
    
    Returns: 
        (success_count, rate_limited, errors)
    """
    success = 0
    errors = []
    
    for i, (date_str, reunion_slug, course_file, filepath) in enumerate(courses[:batch_size]):
        # Construire l'URL
        url = build_course_url(date_str, reunion_slug, course_file)
        
        try:
            html, status = await fetch_course(session, url)
            
            # D√©tection du rate limiting
            if status == 429:
                print(f"      ‚ö†Ô∏è  Rate limit 429 d√©tect√© √† la course {i+1}/{batch_size}")
                return success, True, errors
            
            if status == 200:
                save_html(filepath, html)
                success += 1
                print(f"      ‚úì {course_file}")
            else:
                errors.append(f"{course_file} (HTTP {status})")
                print(f"      ‚úó {course_file} (HTTP {status})")
        
        except Exception as e:
            errors.append(f"{course_file} ({str(e)[:50]})")
            print(f"      ‚úó {course_file} (Error: {str(e)[:50]})")
        
        # Petit d√©lai entre les requ√™tes
        await asyncio.sleep(0.3)
    
    return success, False, errors

# =========================
# Scraping par ann√©e avec auto-ajustement
# =========================
async def scrape_year(year: str, dates_courses: dict, initial_batch_size: int):
    """
    Scrape toutes les courses manquantes pour une ann√©e avec batch size adaptatif.
    
    Args:
        year: Ann√©e √† traiter
        dates_courses: dict[date] = [(reunion_slug, course_file), ...]
        initial_batch_size: Taille initiale des batchs
    """
    global MAX_SAFE_BATCH_SIZE
    
    print(f"\n{'='*80}")
    print(f"ANN√âE {year}")
    print(f"{'='*80}\n")
    
    # V√©rifier l'espace disque avant de commencer
    free_gb = get_disk_space_gb()
    print(f"üíæ Espace disque disponible: {free_gb:.2f} GB")
    
    if free_gb < 3:
        print(f"‚ö†Ô∏è  Espace insuffisant pour traiter cette ann√©e")
        return
    
    # Aplatir toutes les courses pour cette ann√©e
    all_courses = []
    for date_str, courses_list in sorted(dates_courses.items()):
        for reunion_slug, course_file in courses_list:
            date_dir = get_date_directory(date_str)
            reunion_dir = date_dir / reunion_slug
            filepath = reunion_dir / course_file
            all_courses.append((date_str, reunion_slug, course_file, filepath))
    
    total_courses = len(all_courses)
    print(f"üìä {total_courses} courses √† r√©cup√©rer pour {year}")
    
    if total_courses == 0:
        return
    
    # Statistiques
    stats = {
        "success": 0,
        "failed": 0,
        "rate_limits": 0,
        "batch_increases": 0,
        "batch_decreases": 0
    }
    
    batch_size = initial_batch_size
    position = 0
    consecutive_successes = 0  # Compteur pour l'auto-ajustement
    
    async with aiohttp.ClientSession() as session:
        while position < total_courses:
            # V√©rifier l'espace disque avant chaque batch
            if check_disk_space_critical():
                print(f"‚ö†Ô∏è  Arr√™t √† la position {position}/{total_courses}")
                break
            
            remaining = total_courses - position
            current_batch_size = min(batch_size, remaining)
            
            # Afficher le statut
            free_gb = get_disk_space_gb()
            print(f"\n  üì¶ Batch: courses {position+1}-{position+current_batch_size}/{total_courses} (size: {current_batch_size})")
            print(f"  üíæ Espace libre: {free_gb:.2f} GB")
            
            # Traiter le batch
            batch = all_courses[position:position+current_batch_size]
            success, rate_limited, errors = await scrape_courses_batch(session, batch, current_batch_size)
            
            stats["success"] += success
            stats["failed"] += len(errors)
            
            if rate_limited:
                # Rate limit d√©tect√©
                stats["rate_limits"] += 1
                consecutive_successes = 0
                
                # M√©moriser la limite safe (taille actuelle - 10)
                if MAX_SAFE_BATCH_SIZE is None or batch_size - INCREMENT_STEP < MAX_SAFE_BATCH_SIZE:
                    MAX_SAFE_BATCH_SIZE = batch_size - INCREMENT_STEP
                    print(f"      üìå Limite safe d√©tect√©e: {MAX_SAFE_BATCH_SIZE}")
                
                # R√©duire la taille du batch
                batch_size = max(MIN_BATCH_SIZE, batch_size - INCREMENT_STEP)
                stats["batch_decreases"] += 1
                print(f"      üîΩ R√©duction batch size: {batch_size}")
                print(f"      ‚è∏Ô∏è  Attente 30s avant retry...")
                await asyncio.sleep(30)
                
                # Ne pas incr√©menter position - retry le m√™me batch
                continue
            
            # Batch r√©ussi
            consecutive_successes += 1
            
            # Tentative d'augmentation si 3 succ√®s cons√©cutifs
            if consecutive_successes >= CONSECUTIVE_THRESHOLD:
                can_increase = True
                
                # Ne pas d√©passer la limite safe connue
                if MAX_SAFE_BATCH_SIZE is not None:
                    if batch_size >= MAX_SAFE_BATCH_SIZE:
                        can_increase = False
                        print(f"      ‚ÑπÔ∏è  Batch size au maximum safe ({MAX_SAFE_BATCH_SIZE})")
                
                if can_increase:
                    batch_size += INCREMENT_STEP
                    consecutive_successes = 0
                    stats["batch_increases"] += 1
                    print(f"      üîº Augmentation batch size: {batch_size}")
            
            # Passer au batch suivant
            position += current_batch_size
            
            # Petit d√©lai entre les batchs
            if position < total_courses:
                await asyncio.sleep(2)
    
    # Afficher le r√©sum√©
    print(f"\n{'='*80}")
    print(f"R√âSUM√â ANN√âE {year}")
    print(f"{'='*80}")
    print(f"‚úì Succ√®s:          {stats['success']}/{total_courses}")
    print(f"‚úó √âchecs:          {stats['failed']}")
    print(f"‚ö†Ô∏è  Rate limits:     {stats['rate_limits']}")
    print(f"üîº Augmentations:   {stats['batch_increases']}")
    print(f"üîΩ R√©ductions:      {stats['batch_decreases']}")
    if MAX_SAFE_BATCH_SIZE is not None:
        print(f"üìå Max safe size:   {MAX_SAFE_BATCH_SIZE}")
    print(f"üíæ Espace final:    {get_disk_space_gb():.2f} GB")
    print(f"{'='*80}\n")

# =========================
# Git operations
# =========================
def git_commit_year(year: str):
    """Commit et push les changements pour l'ann√©e"""
    print(f"\nüì§ Git commit pour l'ann√©e {year}...")
    try:
        subprocess.run(["git", "config", "user.name", "GitHub Actions Bot"], check=True)
        subprocess.run(["git", "config", "user.email", "actions@github.com"], check=True)
        
        # V√©rifier s'il y a des changements
        result = subprocess.run(["git", "status", "--porcelain"], capture_output=True, text=True)
        if not result.stdout.strip():
            print("  ‚ÑπÔ∏è  Aucun changement pour cette ann√©e")
            return
        
        subprocess.run(["git", "add", f"{REPO_ROOT}/{year}"], check=True)
        
        # Compter les fichiers ajout√©s
        files_added = result.stdout.count('\n')
        
        subprocess.run([
            "git", "commit", "-m", 
            f"Re-scrape: {year} - {files_added} courses ajout√©es",
            "-m", f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}"
        ], check=True)
        subprocess.run(["git", "push"], check=True)
        print(f"  ‚úì Ann√©e {year} committ√©e ({files_added} fichiers)\n")
    except subprocess.CalledProcessError as e:
        print(f"  ‚úó Erreur Git: {e}\n")

# =========================
# Main orchestrator
# =========================
async def main():
    import argparse
    parser = argparse.ArgumentParser(
        description="Re-scrape intelligent des courses ZEturf manquantes"
    )
    parser.add_argument(
        "--max-courses", 
        type=int, 
        default=None, 
        help="Limite globale de courses √† traiter"
    )
    parser.add_argument(
        "--batch-size", 
        type=int, 
        default=INITIAL_BATCH_SIZE, 
        help="Taille initiale des batchs (d√©faut: 200)"
    )
    args = parser.parse_args()
    
    print("="*80)
    print("RE-SCRAPING DIRECT DES COURSES MANQUANTES")
    print("="*80 + "\n")
    
    # V√©rification initiale de l'espace disque
    free_gb = get_disk_space_gb()
    print(f"üíæ Espace disque initial: {free_gb:.2f} GB\n")
    
    if free_gb < 5:
        print("‚ö†Ô∏è  WARNING: Espace disque faible! Recommand√©: > 5GB")
        print("Continuation avec prudence...\n")
    
    # Parser le rapport
    missing_by_year = parse_missing_courses()
    
    if not missing_by_year:
        print("‚úì Aucune course manquante d√©tect√©e\n")
        return
    
    # R√©sum√©
    total_courses = sum(
        len(courses)
        for year_data in missing_by_year.values()
        for courses in year_data.values()
    )
    print(f"üìä {len(missing_by_year)} ann√©es avec courses manquantes")
    print(f"üìä {total_courses} courses manquantes au total\n")
    
    # Traiter ann√©e par ann√©e
    courses_processed = 0
    for year in sorted(missing_by_year.keys()):
        # V√©rifier l'espace disque avant chaque ann√©e
        free_gb = get_disk_space_gb()
        if free_gb < 2:
            print(f"‚ö†Ô∏è  ARR√äT: Espace disque insuffisant ({free_gb:.2f} GB)")
            print(f"Progression: {courses_processed}/{total_courses} courses trait√©es")
            break
        
        # V√©rifier la limite globale
        if args.max_courses and courses_processed >= args.max_courses:
            print(f"‚ö†Ô∏è  Limite globale atteinte ({args.max_courses} courses)")
            break
        
        # Scraper l'ann√©e
        await scrape_year(year, missing_by_year[year], args.batch_size)
        
        # Committer pour cette ann√©e
        git_commit_year(year)
        
        # Mettre √† jour le compteur
        year_courses = sum(len(c) for c in missing_by_year[year].values())
        courses_processed += year_courses
    
    print("\n" + "="*80)
    print("SCRAPING TERMIN√â")
    print(f"üíæ Espace disque final: {get_disk_space_gb():.2f} GB")
    print("="*80)

if __name__ == "__main__":
    asyncio.run(main())
